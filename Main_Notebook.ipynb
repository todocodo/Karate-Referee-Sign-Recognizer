{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c865ad",
   "metadata": {},
   "source": [
    "# Main Notebook\n",
    "\n",
    "The model was trained in \"training_model.ipynb\" file in the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5777cc9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.4.1 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (2.4.1)\n",
      "Requirement already satisfied: tensorflow-gpu==2.4.1 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (2.4.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (4.5.4.60)\n",
      "Requirement already satisfied: mediapipe in c:\\programdata\\anaconda3\\lib\\site-packages (0.8.8.1)\n",
      "Requirement already satisfied: sklearn in c:\\programdata\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.4.1) (3.3.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.15.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.4.1) (1.12)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.4.1) (1.1.2)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (1.15.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.4.1) (1.12.1)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (2.10.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.4.1) (1.32.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (3.18.1)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.4.1) (1.1.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.4.1) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.4.1) (0.3.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.4.1) (2.7.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (0.36.2)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.4.1) (3.7.4.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.4.1) (1.19.5)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.4.1) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.4.1) (2.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\asus\\appdata\\roaming\\python\\python38\\site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.1.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (4.5.3.56)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (20.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e4f0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c3c38",
   "metadata": {},
   "source": [
    "# Add Mediapipe model - Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7a078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3d4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    \n",
    "    # Colour Detection - START \n",
    "    \n",
    "    color_str = []\n",
    "   \n",
    "    try:\n",
    "        for coords in coords_arr:\n",
    "            #Need to check whether it gets the real colors from the video\n",
    "            coords_revert = coords[::-1]\n",
    "            color = image[coords_revert]\n",
    "        #     color = image[123,903]\n",
    "        #     color = image[coords_shoulder]\n",
    "            r = color[2]\n",
    "            g = color[1]\n",
    "            b = color[0]\n",
    "\n",
    "\n",
    "            color_str.append(recognize_color(r,g,b))\n",
    "    except:\n",
    "        pass\n",
    "    # Colour Detection - END\n",
    "    \n",
    "    return image, results, color_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1b6b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c224600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae14d87",
   "metadata": {},
   "source": [
    "#  Action Detection - Model uploaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27a01288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the actions array for the different actions recognized\n",
    "\n",
    "# actions = np.array(['Point', 'Penalty', 'Ignore'])\n",
    "\n",
    "actions = np.array(['Point', 'Penalty', 'Ignore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f3532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "318ae1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2a45314",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be739d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Model - It needs to be the same model structure / architecture as the one that needs to be loaded\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,258)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9193f",
   "metadata": {},
   "source": [
    "#### Load Different Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c1912b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model is already trained then it only needs to be loaded \n",
    "# check actionDetection folder for the training\n",
    "\n",
    "model.load_weights('actionDetection_3.h5')\n",
    "# model.load_weights('actionDetection_4Classes_01.h5')\n",
    "# model.load_weights('actionDetection_50P.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3e7a651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 30, 64)            82688     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 237,251\n",
      "Trainable params: 237,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77d3609",
   "metadata": {},
   "source": [
    "# Differentiation between a referee and a player "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12826b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "index_blue=[\"color_name\", \"R\", \"G\", \"B\"]\n",
    "csv_blue = pd.read_csv('colors_blue.csv', delimiter=\";\", names=index_blue, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12412960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       color_name    R    G    B\n",
      "0           Black    0    0    0\n",
      "1    Darkest Blue    8   22   39\n",
      "2            Navy   17   46   81\n",
      "3       Dark Blue   32   84  147\n",
      "4            Blue   46  120  210\n",
      "5      Light Blue  109  161  224\n",
      "6    Lighter Blue  151  188  233\n",
      "7   Lightest Blue  193  215  242\n",
      "8            Grey  120  144  156\n",
      "9      Light Grey  167  192  205\n",
      "10   Lighter Grey  200  215  223\n",
      "11  Lightest Grey  232  293  242\n"
     ]
    }
   ],
   "source": [
    "print(csv_blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97f3a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Could be either .all() or any()\n",
    "\n",
    "# If we want to recognize only Blue then we need a different color dataset \n",
    "# If we want to recognize only White then we only need the sum of all r,g,b values to be non-less then 690\n",
    "\n",
    "# Calculating the distance from the actual colors from the dataset \n",
    "# and whichever has the lowest distance is assigned \n",
    "def recognize_color(r,g,b):\n",
    " \n",
    "    minimum = 10000 \n",
    "    cname = \"\"\n",
    "    for i in range(len(csv_blue)):\n",
    "        distance = abs(r - int(csv_blue.loc[i,\"R\"])) + abs(g - int(csv_blue.loc[i,\"G\"])) + abs(b - int(csv_blue.loc[i,\"B\"]))\n",
    "        if (distance <= minimum).all():\n",
    "            minimum = distance\n",
    "            cname = csv_blue.loc[i,\"color_name\"]\n",
    "    return cname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c1929d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_detection(image, landmarks, color_str):\n",
    "    coords_shoulder = tuple(np.multiply(\n",
    "            np.array(\n",
    "                (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER].x, \n",
    "                 results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER].y))\n",
    "        , [1280,720]).astype(int))\n",
    "\n",
    "    coords_right_hip = tuple(np.multiply(\n",
    "            np.array(\n",
    "                (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_HIP].x, \n",
    "                 results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_HIP].y))\n",
    "        , [1280,720]).astype(int))\n",
    "\n",
    "    coords_left_hip = tuple(np.multiply(\n",
    "            np.array(\n",
    "                (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_HIP].x, \n",
    "                 results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_HIP].y))\n",
    "        , [1280,720]).astype(int))\n",
    "\n",
    "    coords_arr = [coords_shoulder, coords_right_hip, coords_left_hip]\n",
    "\n",
    "\n",
    "    if \"Navy\" in color_str or \"Darkest Blue\" in color_str or \"Dark Blue\" in color_str:\n",
    "\n",
    "        cv2.rectangle(image, (590, 570), (800, 610), (0, 255, 0), -1)\n",
    "        cv2.putText(image, \"A Referee\", (600, 600), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    else:\n",
    "        cv2.rectangle(image, (590, 570), (800, 610), (0, 0, 255), -1)\n",
    "        cv2.putText(image, \"A Player\", (600, 600), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.putText(image, \"S - {}, LH - {}, RH - {}\".format(color_str[0], color_str[1], color_str[2]), (300, 700), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4bf9178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method for combining all three colours into one number that can be compared. \n",
    "\n",
    "def distinguish_referee(color_str):\n",
    "    \n",
    "    amount = 0 \n",
    "    \n",
    "    for color in color_str:\n",
    "        if color == \"Black\":\n",
    "            amount+= 100\n",
    "        if color == \"Darkest Blue\":\n",
    "            amount+=90\n",
    "        if color == \"Navy\":\n",
    "            amount+=80\n",
    "        if color == \"Dark Blue\":\n",
    "            amount+=70\n",
    "\n",
    "        if color == \"Grey\":\n",
    "            amount+=40\n",
    "        if color == \"Light Grey\":\n",
    "            amount+=30\n",
    "        if color == \"Lighter Grey\":\n",
    "            amount+=20\n",
    "        if color == \"Lightest Grey\":\n",
    "            amount+=10\n",
    "            \n",
    "    return amount\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd2c25",
   "metadata": {},
   "source": [
    "# Calculating & Visualising Anlges  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66c607",
   "metadata": {},
   "source": [
    "Angles of the elbows (between hand and shoulder) could also be calcuated. This would help to differentiate between a point and a penalty from category 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5596b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    \n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6660010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(landmarks, image):\n",
    "    # Get coordinates for Left Hand\n",
    "    hip = [landmarks[mp_holistic.PoseLandmark.LEFT_HIP.value].x,landmarks[mp_holistic.PoseLandmark.LEFT_HIP.value].y]\n",
    "    shoulder = [landmarks[mp_holistic.PoseLandmark.LEFT_SHOULDER.value].x,landmarks[mp_holistic.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "    elbow = [landmarks[mp_holistic.PoseLandmark.LEFT_ELBOW.value].x,landmarks[mp_holistic.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "\n",
    "    # Get coordinates for Right Hand\n",
    "    hip_right = [landmarks[mp_holistic.PoseLandmark.RIGHT_HIP.value].x,landmarks[mp_holistic.PoseLandmark.RIGHT_HIP.value].y]\n",
    "    shoulder_right = [landmarks[mp_holistic.PoseLandmark.RIGHT_SHOULDER.value].x,landmarks[mp_holistic.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "    elbow_right = [landmarks[mp_holistic.PoseLandmark.RIGHT_ELBOW.value].x,landmarks[mp_holistic.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "    \n",
    "    # Calculate angles\n",
    "    angle = calculate_angle(hip, shoulder, elbow)\n",
    "    angle_right = calculate_angle(hip_right, shoulder_right, elbow_right)\n",
    "    \n",
    "    #Position of displaying the angle for the Left Side\n",
    "    angle_position_list = list(np.multiply(shoulder, [1280, 720]).astype(int))\n",
    "    angle_position_list[0] = angle_position_list[0]+30 #Placing it 30px on the right\n",
    "    angle_position_tuple = tuple(angle_position_list)\n",
    "\n",
    "    #Position of displaying the angle for the Right Side\n",
    "    angle_right_position_list = list(np.multiply(shoulder_right, [1280, 720]).astype(int))\n",
    "    angle_right_position_list[0] = angle_right_position_list[0]-30 #Placing it 30px on the left\n",
    "    angle_right_position_tuple = tuple(angle_right_position_list)\n",
    "\n",
    "    # Visualize angle\n",
    "    cv2.putText(image, str(angle.astype(int)) + \" C\", \n",
    "                   angle_position_tuple, \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.putText(image, str(angle_right.astype(int)) + \" C\", \n",
    "                   angle_right_position_tuple, \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    \n",
    "    return angle, angle_right\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8663886",
   "metadata": {},
   "source": [
    "# Points Deduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16960e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_deduction(angle, angle_right, points):\n",
    "    # For the Left Hand \n",
    "    if angle >= 20 and angle < 60:\n",
    "        points = \"1_P Left\"\n",
    "\n",
    "    elif angle >= 60 and angle < 110:\n",
    "        points = \"2_P Left\"\n",
    "\n",
    "    elif angle >= 110:\n",
    "        points = \"3_P Left\"\n",
    "\n",
    "    #For the Right Hand \n",
    "    if angle_right >= 20 and angle_right < 60:\n",
    "        points = \"1_P Right\"\n",
    "\n",
    "    elif angle_right >= 60 and angle_right < 110:\n",
    "        points = \"2_P Right\"\n",
    "\n",
    "    elif angle_right >= 110:\n",
    "        points = \"3_P Right\"\n",
    "        \n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9971ac8a",
   "metadata": {},
   "source": [
    "# Scoreboard \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b847e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the size of the scoreboard without the penalty is with 50 lower (in height)\n",
    "\n",
    "def scoreboard_design(image, count_left, count_right):\n",
    "\n",
    "    cv2.rectangle(image, (1040,620), (1150, 720), (255, 0, 0), -1)\n",
    "    cv2.rectangle(image, (1150, 620), (1260, 720), (0, 0, 255), -1)\n",
    "    cv2.rectangle(image, (1115, 600), (1185, 620), (255, 255, 255), -1)\n",
    "    \n",
    "    cv2.putText(image, \"Score\", \n",
    "           (1130,614), \n",
    "           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "    cv2.putText(image, str(count_left), \n",
    "           (1090,650), \n",
    "           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.putText(image, str(count_right), \n",
    "           (1200,650), \n",
    "           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.line(image, (1040, 670), (1260, 670), (255, 255, 255), 2, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33061d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     if (threes == 1 and predictions[-controlSize:][0] == 3) or (twos == 1 and predictions[-controlSize:][0] == 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f876f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreboard_calc(points, countPoints_left, countPoints_right, controlSize, predictions):\n",
    "    zeros = 0\n",
    "    ones = 0\n",
    "    twos = 0\n",
    "    threes = 0\n",
    "    \n",
    "    # controlSize = 10 by default\n",
    "\n",
    "    for i in range(controlSize):\n",
    "        if predictions[-controlSize:][i] == 0:\n",
    "            zeros += 1\n",
    "        elif predictions[-controlSize:][i] == 1:\n",
    "            ones += 1\n",
    "        elif predictions[-controlSize:][i] == 2:\n",
    "            twos += 1\n",
    "        elif predictions[-controlSize:][i] == 3:\n",
    "            threes += 1\n",
    "\n",
    "    # It needs to check if the ignore is only 1 and then if this ignore is in the first position \n",
    "    # if it doesn't check the number of ignores in this list it will enter the if as many times as \n",
    "    # it has ignores\n",
    "    \n",
    "    # If we want to change to look no at the \"Ignore\", but some other signal - change both values \n",
    "    # in the if statement\n",
    "\n",
    "    if ones == 1 and predictions[-controlSize:][0] == 1:\n",
    "        if points == \"1_P Left\":\n",
    "            countPoints_left += 1\n",
    "        elif points == \"2_P Left\": \n",
    "            countPoints_left += 2\n",
    "        elif points == \"3_P Left\": \n",
    "            countPoints_left += 3\n",
    "        elif points == \"1_P Right\":\n",
    "            countPoints_right += 1\n",
    "        elif points == \"2_P Right\": \n",
    "            countPoints_right += 2\n",
    "        elif points == \"3_P Right\": \n",
    "            countPoints_right += 3\n",
    "    \n",
    "    return countPoints_left, countPoints_right  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9a668",
   "metadata": {},
   "source": [
    "### Rendering & Visualization of the Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2b8b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9de36b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "# colors = [(245,117,16), (117,245,16), (255, 0, 137), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20adef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_Karate_Action_Points = \"France_Spain_2010_Action_Points.mp4\"\n",
    "video_Karate_24 = \"KarateProject_Todor_24_No_Detection.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5afdee1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280.0\n",
      "720.0\n",
      "1280.0\n",
      "720.0\n",
      "25.0\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n",
      "Penalty\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.7\n",
    "\n",
    "# Points recognized based on the angles\n",
    "points = \"\"\n",
    "penalty = \"\"\n",
    "\n",
    "#Scoreboard\n",
    "countPoints_left = 0\n",
    "countPoints_right = 0\n",
    "frameCounter_fps = 0\n",
    "FrameCounter = 0\n",
    "controlSize = 6 #This is really important - it should differ for different output\n",
    "\n",
    "# Colour Detection\n",
    "coords_arr = []\n",
    "color_str = []\n",
    "color_amount = 0\n",
    "\n",
    "cap = cv2.VideoCapture(0) # Set the \"0\" value to \"video_Karate_Action_Points\" to try with a video\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    \n",
    "    #this depends on the output video that is going to be put | the video and the internal camera need to be with the same size\n",
    "    cap.set(3, 1280)\n",
    "    cap.set(4, 720)\n",
    "    \n",
    "    print(cap.get(3))\n",
    "    print(cap.get(4))\n",
    "    print(cap.get(cv2.CAP_PROP_FPS))\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "\n",
    "        # Make detections\n",
    "        image, results, color_str = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        #This code is only for Legend videos - to focus the mediapipe model in the centre - to the referee\n",
    "#         image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) \n",
    "#         results = holistic.process(image[50:500, 310:800])                 \n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "#         print(results)\n",
    "        \n",
    "        # Calculate the angles between the hip / arm \n",
    "        try:\n",
    "            \n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            angle, angle_right = get_angle(landmarks, image) #Method for calculating the angles and visualising them\n",
    "            \n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Draw landmarks \n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "\n",
    "\n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-5:])[0]==np.argmax(res): # This code is holding the a specific prediction for n frames before it confirms that it is trully the one that we want (better for transitioning between classes)\n",
    "                if res[np.argmax(res)] > threshold:\n",
    "\n",
    "                    # START - Points Deduction\n",
    "\n",
    "                    if actions[np.argmax(res)] == \"Point\": \n",
    "\n",
    "                        #A method for deducting the points received\n",
    "                        points = points_deduction(angle, angle_right, points) \n",
    "\n",
    "                        #START - Scoring Points\n",
    "\n",
    "                        try:\n",
    "                        # A method for updating the scoreboard based on the points received\n",
    "                            countPoints_left, countPoints_right = scoreboard_calc(points, countPoints_left, countPoints_right, controlSize, predictions)\n",
    "\n",
    "                        except:\n",
    "                            pass\n",
    "                        #END - Scoring Points\n",
    "\n",
    "                    else:\n",
    "                        points = \"\"\n",
    "\n",
    "                    # END - Points Deduction\n",
    "\n",
    "# #             Viz probabilities\n",
    "#             image = prob_viz(res, actions, image, colors)\n",
    "        try:\n",
    "\n",
    "            if points != \"\":\n",
    "                cv2.rectangle(image, (1042,580), (1252, 540), (255, 117, 16), -1)\n",
    "                cv2.putText(image, points, (1052,570), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            # Check this \n",
    "            if actions[np.argmax(res)] == \"Penalty\":\n",
    "                cv2.rectangle(image, (1042,580), (1252, 540), (16,117,245), -1)\n",
    "                cv2.putText(image, \"Ignore\", (1052,570), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "            cv2.putText(image, \"Predictions:\", (1052,530), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        except:\n",
    "            pass\n",
    "       \n",
    "    # color detection\n",
    "        \n",
    "        try:\n",
    "            \n",
    "#             color_detection(image, landmarks, color_str)\n",
    "\n",
    "            coords_shoulder = tuple(np.multiply(\n",
    "                    np.array(\n",
    "                        (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER].x, \n",
    "                         results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER].y))\n",
    "                , [1280,720]).astype(int))\n",
    "\n",
    "            coords_right_hip = tuple(np.multiply(\n",
    "                    np.array(\n",
    "                        (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_HIP].x, \n",
    "                         results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_HIP].y))\n",
    "                , [1280,720]).astype(int))\n",
    "\n",
    "            coords_left_hip = tuple(np.multiply(\n",
    "                    np.array(\n",
    "                        (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_HIP].x, \n",
    "                         results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_HIP].y))\n",
    "                , [1280,720]).astype(int))\n",
    "\n",
    "            coords_arr = [coords_shoulder, coords_right_hip, coords_left_hip]\n",
    "            \n",
    "            color_amount = distinguish_referee(color_str)\n",
    "\n",
    "\n",
    "            if color_amount > 170:\n",
    "\n",
    "                cv2.rectangle(image, (590, 670), (800, 710), (0, 255, 0), -1)\n",
    "                cv2.putText(image, \"A Referee\", (600, 700), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                \n",
    " \n",
    "            else:\n",
    "                cv2.rectangle(image, (590, 670), (800, 710), (0, 0, 255), -1)\n",
    "                cv2.putText(image, \"A Player\", (600, 700), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#             cv2.putText(image, \"S - {}, LH - {}, RH - {}\".format(color_str[0], color_str[1], color_str[2]), (300, 700), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "        except:\n",
    "            pass\n",
    "  \n",
    "                \n",
    "#         # Scoreboard - Visualisation\n",
    "        scoreboard_design(image, countPoints_left, countPoints_right)\n",
    "        \n",
    "#         # Frame Counter & Current FPS\n",
    "#         frameCounter_fps += 1\n",
    "#         cv2.putText(image, str(frameCounter_fps), (1050, 550), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "#         if len(predictions) > 14: \n",
    "#             cv2.putText(image, str(predictions[-14:]), (550, 500), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "#         cv2.putText(image, points, (650, 600), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c720c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
